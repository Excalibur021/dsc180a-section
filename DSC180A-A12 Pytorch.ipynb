{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import packags\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1., -1.,  1.], grad_fn=<AddBackward0>)\n",
      "tensor([1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# test binary_quantization function\n",
    "def binary_quantization(x):\n",
    "    # x_back is the actual tensor for gradient computation\n",
    "    # while sign(x) is the forwarded tensor\n",
    "    # detach() can eliminate the gradient\n",
    "    x_back = torch.clamp(x, -1, 1)\n",
    "    return (torch.sign(x) - x_back).detach() + x_back\n",
    "\n",
    "\n",
    "x = torch.tensor([0.4, -0.5, 1.6]).requires_grad_()\n",
    "x_bin = binary_quantization(x)\n",
    "# print the result after binarization\n",
    "print(x_bin)\n",
    "y = x_bin.sum()\n",
    "y.backward()\n",
    "# print the gradient of x\n",
    "print(x.grad)\n",
    "\n",
    "#Feeforwarding: quantize all positive numbers in tensor to 1, and negative numbers to -1,\n",
    "#Backpropagation: results as the gradient of tensor is 1 in the range of -1 to 1, and the rest is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0372,  0.0549, -0.0146],\n",
      "          [ 0.1056, -0.0581,  0.1103],\n",
      "          [-0.0338, -0.0923, -0.0289]],\n",
      "\n",
      "         [[-0.0779, -0.0049,  0.0964],\n",
      "          [ 0.0847, -0.0364, -0.0876],\n",
      "          [ 0.0906, -0.0513, -0.0765]],\n",
      "\n",
      "         [[-0.1091, -0.0264,  0.0387],\n",
      "          [-0.0352, -0.0145, -0.0646],\n",
      "          [ 0.0448,  0.1037,  0.0030]],\n",
      "\n",
      "         [[-0.0483,  0.0579,  0.1061],\n",
      "          [ 0.1437, -0.0604, -0.1023],\n",
      "          [ 0.1088, -0.1454, -0.0817]],\n",
      "\n",
      "         [[-0.0568,  0.0379, -0.0997],\n",
      "          [ 0.0092, -0.0136, -0.1254],\n",
      "          [-0.0279,  0.0871, -0.1239]]],\n",
      "\n",
      "\n",
      "        [[[-0.0647,  0.0777, -0.0253],\n",
      "          [-0.1367,  0.1162,  0.0514],\n",
      "          [-0.0803, -0.0061, -0.0370]],\n",
      "\n",
      "         [[ 0.0755,  0.0176,  0.1163],\n",
      "          [-0.1432, -0.1001, -0.1034],\n",
      "          [-0.0126,  0.0075, -0.0054]],\n",
      "\n",
      "         [[ 0.0034,  0.1260,  0.1113],\n",
      "          [ 0.0187,  0.1089,  0.1166],\n",
      "          [-0.0460,  0.0260, -0.0046]],\n",
      "\n",
      "         [[ 0.1418,  0.0324, -0.0304],\n",
      "          [ 0.1222,  0.0292,  0.0843],\n",
      "          [-0.0871,  0.0219, -0.0276]],\n",
      "\n",
      "         [[-0.0124,  0.0331,  0.0056],\n",
      "          [ 0.0141,  0.0851, -0.0851],\n",
      "          [ 0.0372, -0.0370, -0.1179]]],\n",
      "\n",
      "\n",
      "        [[[-0.0065,  0.0200, -0.0121],\n",
      "          [ 0.1355,  0.0080, -0.0402],\n",
      "          [-0.0940,  0.0904, -0.0393]],\n",
      "\n",
      "         [[-0.0789, -0.1310, -0.0433],\n",
      "          [ 0.1189, -0.0404, -0.0100],\n",
      "          [ 0.1042, -0.0276,  0.0872]],\n",
      "\n",
      "         [[-0.1089,  0.0784,  0.1055],\n",
      "          [ 0.0692, -0.0400, -0.0038],\n",
      "          [-0.1363, -0.1424, -0.0018]],\n",
      "\n",
      "         [[ 0.1203, -0.0498, -0.1197],\n",
      "          [-0.1392, -0.0341,  0.1439],\n",
      "          [-0.0188,  0.0070, -0.1339]],\n",
      "\n",
      "         [[ 0.1462, -0.1379,  0.0641],\n",
      "          [-0.0296, -0.1194,  0.0016],\n",
      "          [-0.0970,  0.0410, -0.0966]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0934, -0.0901,  0.0991],\n",
      "          [-0.0912, -0.0435,  0.0966],\n",
      "          [-0.0315, -0.0455,  0.1058]],\n",
      "\n",
      "         [[-0.0402,  0.0454, -0.1168],\n",
      "          [-0.0972,  0.0525,  0.0004],\n",
      "          [-0.0045,  0.0958,  0.1079]],\n",
      "\n",
      "         [[ 0.0612, -0.0084,  0.0026],\n",
      "          [ 0.1338, -0.0249, -0.1212],\n",
      "          [-0.1123, -0.1265,  0.1289]],\n",
      "\n",
      "         [[ 0.0587, -0.0040, -0.0631],\n",
      "          [ 0.1482,  0.1470, -0.0174],\n",
      "          [ 0.1375,  0.0784,  0.0012]],\n",
      "\n",
      "         [[-0.1247,  0.1414, -0.1015],\n",
      "          [ 0.1347,  0.0250,  0.1013],\n",
      "          [ 0.1277, -0.0527, -0.0646]]],\n",
      "\n",
      "\n",
      "        [[[-0.0687,  0.1336, -0.1012],\n",
      "          [ 0.1276,  0.1390,  0.0111],\n",
      "          [-0.1334,  0.1464, -0.1070]],\n",
      "\n",
      "         [[-0.0075, -0.0252, -0.0777],\n",
      "          [ 0.0383,  0.0856,  0.1210],\n",
      "          [-0.0078, -0.0260,  0.0331]],\n",
      "\n",
      "         [[-0.0773,  0.0357, -0.0820],\n",
      "          [ 0.1194, -0.0621, -0.0588],\n",
      "          [ 0.0860, -0.0977,  0.0482]],\n",
      "\n",
      "         [[ 0.0871, -0.0175,  0.1311],\n",
      "          [ 0.0643, -0.1115, -0.1016],\n",
      "          [ 0.0186, -0.0299, -0.0048]],\n",
      "\n",
      "         [[-0.1114, -0.0092, -0.0302],\n",
      "          [ 0.0797, -0.1470,  0.0468],\n",
      "          [ 0.1292,  0.1349, -0.1253]]],\n",
      "\n",
      "\n",
      "        [[[-0.0341, -0.0779, -0.1319],\n",
      "          [ 0.0971, -0.1131,  0.0849],\n",
      "          [-0.0554, -0.0384,  0.0577]],\n",
      "\n",
      "         [[-0.0966, -0.1375,  0.0584],\n",
      "          [ 0.0517, -0.0177,  0.1428],\n",
      "          [-0.1361,  0.1015, -0.1272]],\n",
      "\n",
      "         [[-0.0482, -0.0184, -0.1288],\n",
      "          [-0.0366, -0.1052, -0.1067],\n",
      "          [-0.1227,  0.0113, -0.1067]],\n",
      "\n",
      "         [[ 0.0875, -0.0212, -0.1223],\n",
      "          [-0.0159, -0.0751, -0.0282],\n",
      "          [ 0.0973, -0.0793,  0.0425]],\n",
      "\n",
      "         [[-0.0927, -0.0345,  0.0382],\n",
      "          [-0.0625,  0.1085,  0.0641],\n",
      "          [-0.1353, -0.1246, -0.1065]]],\n",
      "\n",
      "\n",
      "        [[[-0.1429, -0.1490,  0.1395],\n",
      "          [ 0.0190,  0.1371,  0.1259],\n",
      "          [-0.0148, -0.0339,  0.0817]],\n",
      "\n",
      "         [[-0.1112, -0.1457, -0.0028],\n",
      "          [-0.1403,  0.0254, -0.0107],\n",
      "          [ 0.0862, -0.0014,  0.0995]],\n",
      "\n",
      "         [[-0.1236, -0.0596, -0.0338],\n",
      "          [-0.0715, -0.1047,  0.0045],\n",
      "          [-0.0626, -0.0468,  0.1012]],\n",
      "\n",
      "         [[ 0.1407,  0.0672,  0.0396],\n",
      "          [-0.1489,  0.0611, -0.0680],\n",
      "          [ 0.0294,  0.0642, -0.0923]],\n",
      "\n",
      "         [[-0.0145, -0.0162, -0.0818],\n",
      "          [ 0.1396, -0.1418,  0.0780],\n",
      "          [-0.1141, -0.1461, -0.1126]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0535, -0.0322,  0.0586],\n",
      "          [ 0.1406,  0.1172, -0.1430],\n",
      "          [ 0.0734,  0.0492, -0.0507]],\n",
      "\n",
      "         [[-0.0028, -0.1220,  0.0956],\n",
      "          [ 0.0208, -0.0357, -0.1069],\n",
      "          [-0.0681, -0.0627, -0.0150]],\n",
      "\n",
      "         [[-0.1038,  0.0169,  0.0504],\n",
      "          [ 0.1409,  0.0606, -0.0669],\n",
      "          [-0.0981, -0.0361,  0.1259]],\n",
      "\n",
      "         [[-0.1353,  0.0906,  0.0840],\n",
      "          [ 0.1226, -0.0704,  0.0987],\n",
      "          [-0.1444,  0.0715, -0.0778]],\n",
      "\n",
      "         [[ 0.1262, -0.0943, -0.0385],\n",
      "          [ 0.1436,  0.0883,  0.0624],\n",
      "          [-0.0715,  0.0288, -0.0806]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1327,  0.0513,  0.0369],\n",
      "          [ 0.0024,  0.0400, -0.0388],\n",
      "          [-0.0924,  0.0603,  0.0012]],\n",
      "\n",
      "         [[ 0.0058, -0.0967,  0.0243],\n",
      "          [ 0.0261, -0.0164,  0.0408],\n",
      "          [-0.1359,  0.1154,  0.0513]],\n",
      "\n",
      "         [[ 0.1007, -0.0344,  0.0689],\n",
      "          [-0.0748, -0.1131, -0.0251],\n",
      "          [-0.0992,  0.1286, -0.0154]],\n",
      "\n",
      "         [[-0.0675,  0.0490, -0.0538],\n",
      "          [ 0.0337, -0.0952,  0.1172],\n",
      "          [ 0.0403, -0.0897,  0.1282]],\n",
      "\n",
      "         [[-0.0345,  0.0935, -0.0050],\n",
      "          [-0.1416,  0.1052,  0.1437],\n",
      "          [ 0.0964, -0.0599,  0.1164]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1019, -0.0240, -0.1485],\n",
      "          [-0.0181,  0.0432,  0.0134],\n",
      "          [-0.0044,  0.0759, -0.1046]],\n",
      "\n",
      "         [[ 0.1398,  0.0453, -0.0391],\n",
      "          [ 0.0629, -0.0934,  0.0324],\n",
      "          [ 0.1042, -0.1163, -0.1062]],\n",
      "\n",
      "         [[ 0.1054, -0.0902,  0.0692],\n",
      "          [-0.1152,  0.1336,  0.0796],\n",
      "          [ 0.0450, -0.1354, -0.0574]],\n",
      "\n",
      "         [[ 0.0117,  0.1156, -0.0624],\n",
      "          [-0.0722,  0.0328, -0.0666],\n",
      "          [ 0.1403,  0.1026, -0.1084]],\n",
      "\n",
      "         [[ 0.0484,  0.0133, -0.0558],\n",
      "          [-0.0384,  0.1202,  0.0896],\n",
      "          [ 0.0551, -0.1385,  0.0707]]]])\n",
      "tensor([[[[-1.,  1., -1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [-1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [ 1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1., -1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1., -1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [-1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1., -1.,  1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1., -1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.]]]])\n"
     ]
    }
   ],
   "source": [
    "# test binary convolutional function\n",
    "# define binary convolutional layer\n",
    "class BinConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False):\n",
    "        super(BinConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups,\n",
    "                                        bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = binary_quantization(self.weight)\n",
    "        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "# define a Binary convolution in input channel = 5，output channel = 10，kernelsize = 3\n",
    "binary_conv = BinConv2d(5, 10, 3, 1)\n",
    "print(binary_conv.weight.data)   # Print the value of convolution\n",
    "print(binary_quantization(binary_conv.weight.data))   # Print the binarized convolution, in weight value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1304, -0.1292,  0.0275, -0.3544, -0.4292],\n",
      "        [-0.2722, -0.2451, -0.1060, -0.3585, -0.3061],\n",
      "        [-0.3336, -0.2596,  0.3759, -0.3470, -0.2061],\n",
      "        [ 0.0428, -0.3117,  0.1662, -0.0858,  0.2013],\n",
      "        [-0.1835, -0.3415, -0.2589,  0.1170, -0.0849],\n",
      "        [ 0.2892,  0.3331, -0.2601, -0.3168, -0.3874],\n",
      "        [ 0.2796, -0.1383, -0.4007, -0.2742,  0.4123],\n",
      "        [ 0.4027, -0.1929, -0.2911, -0.0935, -0.0592],\n",
      "        [-0.2384, -0.1725, -0.1989, -0.1880,  0.0932],\n",
      "        [-0.1660,  0.3736, -0.4236,  0.0472, -0.2426]])\n",
      "tensor([[ 1., -1.,  1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1.],\n",
      "        [-1., -1.,  1., -1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.],\n",
      "        [-1., -1., -1.,  1., -1.],\n",
      "        [ 1.,  1., -1., -1., -1.],\n",
      "        [ 1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# test binary fully-connected function\n",
    "# define binary fully-connected layer\n",
    "class BinLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(BinLinear, self).__init__(in_features, out_features, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = binary_quantization(self.weight)\n",
    "        return F.linear(x, weight, self.bias)\n",
    "\n",
    "# Define a binary fully connected layer with an input channel of 5 and an output channel of 10\n",
    "binary_conv = BinLinear(5, 10)\n",
    "print(binary_conv.weight.data)   # Print the value of this fully connected weight\n",
    "print(binary_quantization(binary_conv.weight.data))   # Print the binarized weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5000, -0.5000,  2.0000])\n"
     ]
    }
   ],
   "source": [
    "# define LogAP2 function\n",
    "def LogAP2(x, eps=0.0):\n",
    "    # implement Most Significant Bit according to the paper\n",
    "    return torch.sign(x) * 2 ** (torch.round(torch.log2(x.abs() + eps)))\n",
    "\n",
    "# The purpose of this function is to convert any floating-point number x to its nearest second power 2^n，\n",
    "# That is, for any x, we find n such that the difference between the nth power of 2 and x is the smallest\n",
    "\n",
    "x = torch.tensor([0.4, -0.5, 1.6])\n",
    "print(LogAP2(x))\n",
    "\n",
    "# When x is abnormally small, this function will overflow due to Log, so we add eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1176e-08, -4.5169e-08, -1.5832e-08,  1.8626e-09, -3.7253e-09,\n",
      "        -2.4214e-08, -1.9558e-08,  7.4506e-08,  9.3831e-08, -5.9605e-08,\n",
      "        -2.0489e-08, -2.4214e-08, -6.5193e-09, -3.2131e-08,  1.5367e-08,\n",
      "        -1.2107e-08, -7.4971e-08,  1.8161e-08, -2.6077e-08, -5.4541e-08,\n",
      "        -2.9802e-08, -1.3039e-08, -1.1292e-08, -8.8476e-09, -2.5146e-08,\n",
      "         1.6298e-08,  6.0536e-09,  7.4506e-09,  8.3819e-09,  2.7940e-09,\n",
      "         1.3039e-08,  1.4901e-08, -3.4692e-08, -2.0023e-08, -4.2841e-08,\n",
      "         0.0000e+00,  1.0361e-08,  6.9849e-10, -1.2107e-08,  3.7951e-08,\n",
      "         2.8871e-08,  3.5390e-08, -1.4901e-08,  2.8871e-08,  1.4901e-08,\n",
      "        -2.7940e-08,  1.3737e-08, -2.9104e-09,  1.3039e-08,  6.7288e-08,\n",
      "        -4.8429e-08,  2.4564e-08,  3.3528e-08,  6.7521e-08, -7.4506e-09,\n",
      "        -3.3760e-09,  5.6811e-08,  7.9162e-09, -1.0245e-08,  3.0501e-08,\n",
      "         6.7055e-08,  9.3132e-10, -5.8208e-08,  1.5832e-08])\n",
      "tensor([0.5437, 0.5397, 0.5266, 0.6047, 0.5262, 0.5908, 0.5116, 0.6117, 0.5233,\n",
      "        0.5820, 0.5487, 0.5671, 0.6011, 0.5739, 0.5562, 0.5154, 0.5536, 0.5913,\n",
      "        0.5183, 0.5695, 0.6028, 0.5674, 0.5582, 0.5411, 0.5370, 0.5913, 0.5863,\n",
      "        0.5942, 0.5988, 0.5635, 0.5546, 0.5941, 0.5409, 0.5507, 0.6177, 0.5872,\n",
      "        0.5748, 0.5750, 0.5697, 0.5813, 0.5655, 0.5660, 0.5479, 0.5652, 0.5808,\n",
      "        0.5358, 0.5013, 0.5034, 0.5605, 0.5700, 0.5773, 0.5307, 0.5650, 0.5706,\n",
      "        0.5609, 0.5571, 0.5565, 0.5579, 0.5440, 0.6197, 0.5747, 0.5803, 0.5680,\n",
      "        0.5482])\n",
      "tensor([0.9737, 0.9894, 0.9697, 0.9759, 0.9897, 0.9773, 0.9712, 0.9859, 0.9701,\n",
      "        0.9713, 0.9958, 0.9773, 0.9780, 0.9797, 0.9763, 0.9838, 0.9891, 0.9905,\n",
      "        0.9844, 0.9822, 0.9870, 0.9803, 0.9665, 0.9960, 0.9746, 0.9790, 0.9876,\n",
      "        0.9949, 0.9783, 0.9738, 0.9880, 0.9778, 0.9812, 0.9770, 0.9785, 0.9922,\n",
      "        0.9681, 0.9754, 0.9731, 0.9790, 0.9688, 0.9930, 0.9811, 0.9931, 0.9962,\n",
      "        0.9600, 0.9810, 0.9803, 0.9658, 0.9748, 0.9817, 0.9701, 0.9656, 0.9832,\n",
      "        0.9915, 0.9971, 0.9910, 0.9771, 0.9974, 0.9830, 0.9883, 0.9601, 0.9808,\n",
      "        0.9810])\n"
     ]
    }
   ],
   "source": [
    "class ShiftBatchNorm(nn.BatchNorm1d):\n",
    "    def __init__(self, channel, *args):\n",
    "        super(ShiftBatchNorm, self).__init__(channel, *args)\n",
    "\n",
    "    def round_pass(self, x):\n",
    "        # to make rounded tensor still have gradient\n",
    "        return (x.round() - x) + x\n",
    "\n",
    "    def LogAP2(self, x, eps=0.0):\n",
    "        # implement Most Significant Bit according to the paper\n",
    "        return torch.sign(x) * 2 ** (torch.round(torch.log2(x.abs() + eps)))\n",
    "\n",
    "    def get_var(self, x, mean, channel_dim):\n",
    "        # implement variance with LogAP2()\n",
    "        centerd_mean = x - mean.reshape(1, channel_dim)\n",
    "        variance = centerd_mean * self.LogAP2(centerd_mean, eps=0.0001)\n",
    "        return variance.mean([0])\n",
    "\n",
    "    def forward(self, input, round_var=True):\n",
    "        # determine the average factor and channel dimension\n",
    "        exponential_average_factor = self.momentum\n",
    "        channel_dim = input.shape[1]\n",
    "        # calculate running estimates if the model is in training\n",
    "        if self.training:\n",
    "            mean = input.mean([0])\n",
    "            var = self.get_var(input, mean, channel_dim)\n",
    "\n",
    "            # Update the running stats in Batch Normalization Layer\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = exponential_average_factor * mean + (\n",
    "                            1 - exponential_average_factor) * self.running_mean\n",
    "                # update running_var with unbiased var\n",
    "                self.running_var = exponential_average_factor * var + (\n",
    "                            1 - exponential_average_factor) * self.running_var\n",
    "        # In testing the model directly uses the running stats\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        # calculate the normalization factor\n",
    "        div = 1 / (torch.sqrt(var.reshape(1, channel_dim) + self.eps))\n",
    "        if round_var:\n",
    "            div = self.LogAP2(div)\n",
    "        input = (input - mean.reshape(1, channel_dim)) * div  # normalize\n",
    "        return input\n",
    "    \n",
    "\n",
    "# Initialize the BatchNorm layer（channel=64）\n",
    "bn = ShiftBatchNorm(64)\n",
    "\n",
    "# Randomly create activation (Batchsize=1024, channel=64)\n",
    "x = torch.randn(1024, 64)\n",
    "\n",
    "# turn the average of activation to 3，variation to 9\n",
    "x = x * 3 + 3\n",
    "\n",
    "# Test the performance of BN\n",
    "y = bn(x)\n",
    "# Seperately print the results of average and var\n",
    "print(y.mean(0))\n",
    "print(y.var(0))\n",
    "# The reason why the variance of this is not 1 is that it was originally meant to be divided by 0.333, \n",
    "# but the LogAP2 function changed the number to be divided into 0.25, so the variance is not.\n",
    "# I set a parameter round_var, when it is set to False, the variance can be close to 1:\n",
    "y = bn(x, round_var=False)\n",
    "print(y.var(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyModel(\n",
      "  (conv1): BinLinear(in_features=1024, out_features=2048, bias=True)\n",
      "  (bn1): ShiftBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): BinLinear(in_features=2048, out_features=2048, bias=True)\n",
      "  (bn2): ShiftBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TinyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, binarized=True, res=32, rgb=False):\n",
    "        super().__init__()\n",
    "        # define network structures\n",
    "        # if binarized is set to True,\n",
    "        # we use binary linear layer and\n",
    "        # shift batchnorm layer.\n",
    "        mult = 3 if rgb is True else 1\n",
    "        if binarized is True:\n",
    "            self.conv1 = BinLinear(res * res * mult, 2048)\n",
    "            self.bn1 = ShiftBatchNorm(2048)\n",
    "            self.conv2 = BinLinear(2048, 2048)\n",
    "            self.bn2 = ShiftBatchNorm(2048)\n",
    "        else:\n",
    "            self.conv1 = nn.Linear(res * res * mult, 2048)\n",
    "            self.bn1 = nn.BatchNorm1d(2048)\n",
    "            self.conv2 = nn.Linear(2048, 2048)\n",
    "            self.bn2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Standard Conv-BN-Act block, note that the\n",
    "        # activation is not binarized for the first layer\n",
    "        # according to Section 2.6 in the paper.\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        # Binarize activation\n",
    "        x = binary_quantization(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = binary_quantization(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = TinyModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The storage of FP model is 25.296936000000002.MB\n",
      "The storage of binary model is 0.7905292500000001.MB\n"
     ]
    }
   ],
   "source": [
    "# Model Complexity：\n",
    "\n",
    "# Full Precision Model\n",
    "\n",
    "# count the number of parameter\n",
    "num_param = sum([i.numel() for i in model.parameters()])\n",
    "\n",
    "# turn number of parameter into megabyte:  \n",
    "# *Floating point number 32 bits / 8 bits per byte / 1024 bytes to kB / 1024 bytes to MB\n",
    "MBs = num_param * 32 / 8 / 1000 / 1000\n",
    "print('The storage of FP model is {}.MB'.format(MBs))\n",
    "\n",
    "# The parameter of the binary network is 1/32 of the original network, because it only takes up one bit。\n",
    "print('The storage of binary model is {}.MB'.format(MBs/32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Training function for binary model\n",
    "    :param model: the binary model that needs to be optimized\n",
    "    :param device: usually GPU with cuda\n",
    "    :param train_loader: data loader of training dataset\n",
    "    :param optimizer: Adam optimizer\n",
    "    :param epoch: current training epoch\n",
    "    :return: trained model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # start training iterating\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward, compute loss, backward, update loop\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Test function for binary model\n",
    "    :param model: the binary model that needs to be tested\n",
    "    :param device: usually GPU with cuda\n",
    "    :param test_loader: data loader of testing dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # no gradient signal during testing\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "seed = 1001\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_kwargs = {'batch_size': 128}\n",
    "test_kwargs = {'batch_size': 100}\n",
    "\n",
    "cuda_kwargs = {'num_workers': 1,\n",
    "               'pin_memory': True,\n",
    "               'shuffle': True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------Preparing Dataset ----------------------\n",
    "# Dataset can be chosed from CIFAR10, MNIST, and SVHN\n",
    "Dset = 'MNIST'\n",
    "train_kwargs.update(cuda_kwargs)\n",
    "test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "if Dset == 'MNIST':\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                              transform=transform)\n",
    "    dataset2 = datasets.MNIST('data', train=False,\n",
    "                              transform=transform)\n",
    "    res = 28\n",
    "    rgb = False\n",
    "elif Dset == 'SVHN':\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    dataset = datasets.SVHN('data', download=True, transform=transform)\n",
    "    from torch.utils.data import random_split\n",
    "\n",
    "    val_size = 5000\n",
    "    random_seed = 42\n",
    "    torch.manual_seed(random_seed)\n",
    "    train_size = len(dataset) - val_size\n",
    "    dataset1, dataset2 = random_split(dataset, [train_size, val_size])\n",
    "    res = 32\n",
    "    rgb = True\n",
    "elif Dset == 'CIFAR':\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    dataset1 = datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "    dataset2 = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "    res = 32\n",
    "    rgb = True\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7295ec7c3677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prepare model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTinyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# we use cuda default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# prepare model\n",
    "model = TinyModel(False, res, rgb)\n",
    "model.cuda()  # we use cuda default\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# training for 30 epochs\n",
    "epochs = 100\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epochs, )\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "#for this step, if running code in macOS, might given error; but in windows might not.\n",
    "#there is special situation for CUDA used in different system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is figure part\n",
    "# import packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# total 3 bars (MNIST, SVHN, CIFAR10)\n",
    "N = 3\n",
    "\n",
    "# setup color names\n",
    "color1 = 'firebrick'\n",
    "color2 = 'steelblue'\n",
    "color3 = 'darkcyan'\n",
    "color4 = 'darkorange'\n",
    "\n",
    "# test accuracy obtained from `test.py`,\n",
    "# bar1 is the model been binarized, while\n",
    "# bar2 is the 32-bit floating-point model\n",
    "bar1 = [98.04, 83.02, 62.94]\n",
    "bar2 = [98.52, 86.42, 63.62]\n",
    "\n",
    "# setup figure,\n",
    "fig = plt.figure(dpi=120, figsize=[6, 5])\n",
    "plt.rc(\"xtick\", labelsize=14)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=14)\n",
    "# only one plot here\n",
    "ax = plt.subplot(111)\n",
    "# setup grid in y axis for better visualization\n",
    "ax.grid('on', axis='y', linestyle='-.', zorder=0)\n",
    "ind = np.arange(1, N+1)    # the x locations for the groups\n",
    "width = 0.3         # the width of the bars\n",
    "# plot histogram\n",
    "p1 = ax.bar([i-0.5*width for i in ind], bar1, width, bottom=0, zorder=3, color=color2, alpha=0.8)\n",
    "p2 = ax.bar([i+0.5*width for i in ind], bar2, width, zorder=3, color=color1, alpha=0.6)\n",
    "# remove top right axis for better results\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "\n",
    "# plt.setp(ax.get_xticklabels(), visible=False)\n",
    "ax.tick_params(axis='x', which='both', length=0)\n",
    "# setup x axis tick label\n",
    "ax.set_xticks(range(1, 4))\n",
    "ax.set_xticklabels(['MNIST', 'SVHN', 'CIFAR10'])\n",
    "ax.set_xlim(0.5, 3.8)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=17)\n",
    "ax.legend((p1[0], p2[0]), ('Binary', 'Full Precision'), prop={'size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
